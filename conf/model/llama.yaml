model_name: 'NousResearch/Meta-Llama-3-8B'
use_flash_attention_2: true

lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules:
    - 'q_proj'
    - 'k_proj'
    - 'v_proj'
    - 'o_proj'
    - 'gate_proj'
    - 'up_proj'
    - 'down_proj'

train_dataset_name: 'meta-math/Math-10k'
eval_dataset_name: 'gsm8k'
eval_dataset_config: 'main'
prompt_template: "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n{response}<|eot_id|>"
max_seq_length: 1024

output_dir: './results/llama3_math_AsyFlat'
num_train_epochs: 2
per_device_train_batch_size: 2
per_device_eval_batch_size: 4
gradient_accumulation_steps: 8
learning_rate: 2.0e-4
warmup_ratio: 0.03
lr_scheduler_type: 'cosine'
logging_steps: 10
bf16: true
seed: 42
